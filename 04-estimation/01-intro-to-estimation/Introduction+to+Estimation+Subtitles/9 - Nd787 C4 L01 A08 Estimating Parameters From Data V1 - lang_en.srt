1
00:00:00,000 --> 00:00:02,609
One situation where we want to do

2
00:00:02,609 --> 00:00:06,389
parameter estimation has been coming up with a model for a sensor.

3
00:00:06,389 --> 00:00:07,955
Take a laser rangefinder.

4
00:00:07,955 --> 00:00:11,370
If you mount a laser rangefinder exactly one meter from a wall,

5
00:00:11,369 --> 00:00:13,229
and use it to measure the distance,

6
00:00:13,230 --> 00:00:16,420
you probably won't get a measure distance of exactly one meter.

7
00:00:16,420 --> 00:00:19,650
It will likely be slightly above, or slightly below.

8
00:00:19,649 --> 00:00:23,009
But if you repeat this measurement many times,

9
00:00:23,010 --> 00:00:25,950
you'll likely find that on average the sensor does

10
00:00:25,949 --> 00:00:28,980
measure the correct distance plus or minus some error.

11
00:00:28,980 --> 00:00:31,289
That error is also called sensor noise,

12
00:00:31,289 --> 00:00:36,704
and it's often best modeled as a Gaussian random variable with some mean ideally zero,

13
00:00:36,704 --> 00:00:38,039
and some standard deviation.

14
00:00:38,039 --> 00:00:41,609
In order to use that sensor most effectively,

15
00:00:41,609 --> 00:00:46,244
we need an accurate estimate of what the parameters of that distribution are.

16
00:00:46,244 --> 00:00:52,004
We can't know the exact values of the parameters of a distribution given some data,

17
00:00:52,005 --> 00:00:56,130
but we can compute estimates of the parameters in a variety of ways.

18
00:00:56,130 --> 00:00:59,010
For example, a Gaussian is parameterized, as

19
00:00:59,009 --> 00:01:01,905
we said a minute ago, by mu and sigma squared.

20
00:01:01,905 --> 00:01:06,254
We'll write the estimates of these parameters as x_hat, and sigma_hat squared.

21
00:01:06,254 --> 00:01:10,964
Now, let's imagine that I have some Gaussian that's generating samples from x,

22
00:01:10,965 --> 00:01:14,219
and this is p of x, and we don't know mu, or sigma.

23
00:01:14,219 --> 00:01:18,000
We repeatedly draw from this distribution to collect samples of data.

24
00:01:18,000 --> 00:01:22,519
And you can see we'll likely get more data here near the mean,

25
00:01:22,519 --> 00:01:24,774
and fewer samples out on the tails.

26
00:01:24,775 --> 00:01:27,660
Using a technique called the method of moments,

27
00:01:27,659 --> 00:01:30,420
it's easy to show that if we have samples x_1,

28
00:01:30,420 --> 00:01:32,234
x_2 up to x_n,

29
00:01:32,234 --> 00:01:34,560
then x_hat is equal to one over n,

30
00:01:34,560 --> 00:01:38,040
sum from i equals one to n of x of i.

31
00:01:38,040 --> 00:01:40,950
And sigma_hat squared is equal to one over n,

32
00:01:40,950 --> 00:01:43,635
sum from i equals one to n of x_sub_i,

33
00:01:43,635 --> 00:01:45,825
minus x_hat all squared.

34
00:01:45,825 --> 00:01:50,450
It's quite convenient that the mean of the distribution matches the expected value,

35
00:01:50,450 --> 00:01:53,840
and the variance of the distribution matches that variance.

